{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c25b8b97",
   "metadata": {},
   "source": [
    "# 索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# 加载文档\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58b35927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "splits[:3]\n",
    "\n",
    "# 索引\n",
    "import os\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from ark_embedding import ArkEmbeddings\n",
    "\n",
    "embd = ArkEmbeddings(\n",
    "    model=os.getenv(\"ALIYUN_EMBEDDING_MODEL\"),\n",
    "    api_key=os.getenv(\"ALIYUN_API_KEY\"),\n",
    "    api_url=os.getenv(\"ALIYUN_API_URL\"),\n",
    "    batch_size=10\n",
    ")\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embd\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3c129c",
   "metadata": {},
   "source": [
    "# 提示词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b1715d",
   "metadata": {},
   "source": [
    "## Part 5: Multi Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d1540d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "['What is task decomposition for LLM agents?',\n",
      " 'What are the different methods for breaking down complex tasks into '\n",
      " 'sub-tasks for LLM agents?',\n",
      " 'How do LLM agents use task decomposition to solve problems?',\n",
      " 'Explain the concept and process of task decomposition in the context of '\n",
      " 'large language model agents.',\n",
      " 'What role does task decomposition play in the architecture and functionality '\n",
      " 'of an LLM agent?',\n",
      " 'Why is decomposing a complex goal into smaller steps important for AI agents '\n",
      " 'based on large language models?']\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "\n",
    "# 不同视角的multi query\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=os.getenv(\"ARK_MODEL\"),\n",
    "    api_key=os.getenv(\"ARK_API_KEY\"),\n",
    "    base_url=os.getenv(\"ARK_API_URL\"),\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "# 构建处理链\n",
    "# generate_queries = (\n",
    "#     prompt_perspectives\n",
    "#     | llm\n",
    "#     | StrOutputParser()\n",
    "#     | (lambda x: x.split(\"\\n\"))\n",
    "# )\n",
    "\n",
    "# 构建处理链 - 保留原始查询\n",
    "generate_queries = (\n",
    "    RunnableParallel({\n",
    "        \"original\": RunnablePassthrough(),\n",
    "        \"variations\": (\n",
    "            prompt_perspectives\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "            | (lambda x: x.split(\"\\n\"))\n",
    "        )\n",
    "    })\n",
    "    | (lambda x: [x[\"original\"][\"question\"]] + x[\"variations\"])\n",
    ")\n",
    "\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "multi_queries = generate_queries.invoke({\"question\": question})\n",
    "\n",
    "from pprint import pprint\n",
    "print(len(multi_queries))\n",
    "pprint(multi_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3954e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content=\"Each element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\"),\n",
      " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Resources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.'),\n",
      " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liutingting\\AppData\\Local\\Temp\\ipykernel_8816\\2829956312.py:10: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  unique_union = [loads(unique_doc) for unique_doc in unique_docs]\n"
     ]
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\"\n",
    "    获取唯一并集\n",
    "    \"\"\"\n",
    "    # 使用dumps和loads将文档转换为字符串进行去重，完成后再转换回文档\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    unique_union = [loads(unique_doc) for unique_doc in unique_docs]\n",
    "    return unique_union\n",
    "\n",
    "# 检索\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "retrieval_chain = (\n",
    "    generate_queries \n",
    "    | retriever.map() \n",
    "    | get_unique_union\n",
    ")\n",
    "docs = retrieval_chain.invoke({\"question\": question})\n",
    "len(docs)\n",
    "pprint(docs[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f26db42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Task decomposition for LLM agents is the process of breaking down a large, '\n",
      " 'complex task into smaller, more manageable subgoals or steps. This allows '\n",
      " 'the agent to handle complicated tasks more efficiently by focusing on one '\n",
      " 'smaller step at a time.\\n'\n",
      " '\\n'\n",
      " 'As described in the context, this can be achieved in several ways:\\n'\n",
      " '(1) By instructing the LLM with simple prompting (e.g., \"Steps for '\n",
      " 'XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\"),\\n'\n",
      " '(2) By using task-specific instructions (e.g., \"Write a story outline.\" for '\n",
      " 'writing a novel), or\\n'\n",
      " '(3) With human inputs.\\n'\n",
      " '\\n'\n",
      " 'This technique is inspired by and extends the Chain of Thought (CoT) '\n",
      " 'prompting method, which instructs the model to \"think step by step\" to '\n",
      " 'decompose hard tasks.')\n"
     ]
    }
   ],
   "source": [
    "# RAG\n",
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=os.getenv(\"ARK_MODEL\"),\n",
    "    api_key=os.getenv(\"ARK_API_KEY\"),\n",
    "    base_url=os.getenv(\"ARK_API_URL\"),\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\n",
    "        \"context\": retrieval_chain,\n",
    "        \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = final_rag_chain.invoke({\"question\": question})\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157dd54a",
   "metadata": {},
   "source": [
    "# Part 6: RAG-Fusion(融合)\n",
    "相比multi-queries增加了倒排的环节，但未取topk，实际效果应差不多。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752b899c",
   "metadata": {},
   "source": [
    "## 提示词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5a787f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# RAG-Fusion: Related\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Provide these alternative questions separated by newlines. Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32946027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "['What is task decomposition for LLM agents?',\n",
      " 'What is task decomposition in the context of LLM agents?',\n",
      " 'How do large language models break down complex tasks?',\n",
      " 'Explain the process of task decomposition for AI agents.',\n",
      " 'What are the methods and benefits of task decomposition for LLMs?']\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=os.getenv(\"ARK_MODEL\"),\n",
    "    api_key=os.getenv(\"ARK_API_KEY\"),\n",
    "    base_url=os.getenv(\"ARK_API_URL\"),\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "# 构建处理链 - 保留原始查询\n",
    "generate_queries = (\n",
    "    RunnableParallel({\n",
    "        \"original\": RunnablePassthrough(),\n",
    "        \"variations\": (\n",
    "            prompt_rag_fusion\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "            | (lambda x: x.split(\"\\n\"))\n",
    "        )\n",
    "    })\n",
    "    | (lambda x: [x[\"original\"][\"question\"]] + x[\"variations\"])\n",
    ")\n",
    "\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "multi_queries = generate_queries.invoke({\"question\": question})\n",
    "\n",
    "from pprint import pprint\n",
    "print(len(multi_queries))\n",
    "pprint(multi_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f684f0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory'),\n",
      "  0.08252247488101534),\n",
      " (Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.'),\n",
      "  0.08014152250006296),\n",
      " (Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'),\n",
      "  0.06612021857923497)]\n"
     ]
    }
   ],
   "source": [
    "# 倒排融合\n",
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\"\n",
    "    倒排融合\n",
    "    \"\"\"\n",
    "    fused_scores = {}\n",
    "\n",
    "    for docs in results:\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # 计算得分\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "    # 排序\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "    return reranked_results\n",
    "\n",
    "retrieval_chain_rag_fusion = (\n",
    "    generate_queries\n",
    "    | retriever.map()\n",
    "    | reciprocal_rank_fusion\n",
    ")\n",
    "\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "len(docs)\n",
    "pprint(docs[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fc665b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Task decomposition for LLM agents is the process of breaking down a large, '\n",
      " 'complex task into smaller, more manageable sub-tasks or steps.\\n'\n",
      " '\\n'\n",
      " 'This is a core component of the \"Planning\" module in an LLM-powered '\n",
      " 'autonomous agent system. The technique allows the agent to handle complex '\n",
      " 'tasks more efficiently by focusing on one smaller step at a time. Common '\n",
      " 'methods to achieve task decomposition include:\\n'\n",
      " '\\n'\n",
      " '*   **Chain of Thought (CoT):** Instructing the model to \"think step by '\n",
      " 'step\" to decompose the task.\\n'\n",
      " '*   **Tree of Thoughts (ToT):** Extending CoT by exploring multiple possible '\n",
      " 'reasoning steps at each point, creating a tree of potential paths.\\n'\n",
      " '*   **Simple Prompting:** Using prompts like \"Steps for XYZ\" or \"What are '\n",
      " 'the subgoals for achieving XYZ?\"\\n'\n",
      " '*   **Task-Specific Instructions:** Using instructions tailored to a '\n",
      " 'specific goal, such as \"Write a story outline.\"\\n'\n",
      " '*   **Human Input:** Decomposing tasks based on direct human guidance.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\n",
    "        \"context\": retrieval_chain_rag_fusion,\n",
    "        \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = final_rag_chain.invoke({\"question\": question})\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fe938e",
   "metadata": {},
   "source": [
    "# Part 7: 分解 Decomposition\n",
    "将原问题分解为多个子问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19be6586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Decomposition\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3882c76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. What are the key architectural components of a large language model (LLM) '\n",
      " 'autonomous agent?',\n",
      " '2. How does an autonomous agent system break down a task using an LLM?',\n",
      " '3. What tools and frameworks are commonly used to build LLM-powered '\n",
      " 'autonomous agents?']\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=os.getenv(\"ARK_MODEL\"),\n",
    "    api_key=os.getenv(\"ARK_API_KEY\"),\n",
    "    base_url=os.getenv(\"ARK_API_URL\"),\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "# Chain\n",
    "generate_queries_decomposition = (\n",
    "    prompt_decomposition\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "# Run\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "questions = generate_queries_decomposition.invoke({\"question\":question})\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49bfe84",
   "metadata": {},
   "source": [
    "## 递归地进行回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48384f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f84caef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Based on the provided context, the main components of an LLM-powered '\n",
      " 'autonomous agent system are:\\n'\n",
      " '\\n'\n",
      " \"1.  **LLM Core (The Agent's Brain):** The large language model functions as \"\n",
      " 'the core controller or \"brain\" of the entire agent system.\\n'\n",
      " '2.  **Planning:** This component is responsible for:\\n'\n",
      " '    *   **Subgoal and Decomposition:** Breaking down large, complex tasks '\n",
      " 'into smaller, more manageable subgoals.\\n'\n",
      " '    *   **Reflection and Refinement:** Enabling the agent to perform '\n",
      " 'self-criticism and learn from past mistakes to refine future actions and '\n",
      " 'improve results.\\n'\n",
      " '3.  **Memory:** This component provides the agent with the capability to '\n",
      " 'store and retrieve information. It consists of:\\n'\n",
      " \"    *   **Short-term memory:** This is considered the model's in-context \"\n",
      " 'learning, used for immediate tasks.\\n'\n",
      " '    *   **Long-term memory:** This allows the agent to retain and recall '\n",
      " 'information over extended periods, often by leveraging an external vector '\n",
      " 'store and fast retrieval.\\n'\n",
      " '4.  **Tool Use:** This enables the agent to call external APIs and tools to '\n",
      " 'access information not contained within its pre-trained weights (e.g., '\n",
      " 'current information, code execution capability, access to proprietary data '\n",
      " 'sources).\\n'\n",
      " '\\n'\n",
      " '**Source:** The architectural overview from the provided context document by '\n",
      " 'Lilian Weng.')\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"\n",
    "    格式化问题和答案对\n",
    "    \"\"\"\n",
    "    formatted_string = f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=os.getenv(\"ARK_MODEL\"),\n",
    "    api_key=os.getenv(\"ARK_API_KEY\"),\n",
    "    base_url=os.getenv(\"ARK_API_URL\"),\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "# 递归地回答\n",
    "q_a_pairs = \"\"\n",
    "for q in questions + [question]:\n",
    "    rag_chain = (\n",
    "        {\n",
    "            \"context\": itemgetter(\"question\") | retriever,\n",
    "            \"question\": itemgetter(\"question\"),\n",
    "            \"q_a_pairs\": itemgetter(\"q_a_pairs\"),\n",
    "        }\n",
    "        | decomposition_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    answer = rag_chain.invoke({\"question\": q, \"q_a_pairs\": q_a_pairs})\n",
    "    q_a_pairs += \"\\n---\\n\" + format_qa_pair(q, answer)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e168c24e",
   "metadata": {},
   "source": [
    "## 独立回答（可并行）\n",
    "使用asyncio和ainvoke可以实现并行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74bd061f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:\n",
      "Answer the following question based on this context:\n",
      "\n",
      "{context}\n",
      "\n",
      "Question: {question}\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liutingting\\AppData\\Local\\Temp\\ipykernel_8816\\3386233554.py:22: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(sub_question)\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# RAG prompt\n",
    "prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "print(\"prompt:\")\n",
    "print(prompt.messages[0].prompt.template)\n",
    "print(\"=\"*50)\n",
    "\n",
    "def retrieve_and_rag(question, prompt_rag, sub_question_generator_chain):\n",
    "    \"\"\"子问题进行RAG\"\"\"\n",
    "\n",
    "    # 问题分解\n",
    "    sub_questions = sub_question_generator_chain.invoke({\"question\": question})\n",
    "\n",
    "    # 子问题rag\n",
    "    rag_results = []\n",
    "    for sub_question in sub_questions:\n",
    "        retrieved_docs = retriever.get_relevant_documents(sub_question)\n",
    "\n",
    "        answer = (\n",
    "            prompt_rag\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        answer.invoke({\"context\": retrieved_docs, \"question\": sub_question})\n",
    "        rag_results.append(answer)\n",
    "    \n",
    "    return rag_results, sub_questions\n",
    "\n",
    "answers, questions = retrieve_and_rag(\n",
    "    question,\n",
    "    prompt_rag,\n",
    "    generate_queries_decomposition\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43136195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Based on the provided Q&A pairs, the main components of an LLM-powered '\n",
      " 'autonomous agent system are:\\n'\n",
      " '\\n'\n",
      " '1.  **A Prompt Template:** This is the first component '\n",
      " '(`first=ChatPromptTemplate`), which structures the input for the LLM. It '\n",
      " \"defines the system's role, provides instructions (e.g., to use retrieved \"\n",
      " \"context, be concise), and formats the user's question and any relevant \"\n",
      " 'context.\\n'\n",
      " '2.  **The Large Language Model (LLM) Core:** This is the middle component '\n",
      " '(`middle=[ChatOpenAI]`), which is the reasoning engine of the agent. It '\n",
      " 'processes the formatted prompt from the template to generate responses, make '\n",
      " 'decisions, and perform computations. The configuration (e.g., `model_name`, '\n",
      " '`temperature`) is defined here.\\n'\n",
      " '3.  **An Output Parser:** This is the last component '\n",
      " '(`last=StrOutputParser()`), which takes the raw output from the LLM and '\n",
      " 'transforms it into a final, usable format for the user or the next step in a '\n",
      " 'system.\\n'\n",
      " '\\n'\n",
      " 'These three components form a basic but complete chain for processing a '\n",
      " 'question and generating an answer, which is the foundational architecture '\n",
      " 'for an autonomous agent.')\n"
     ]
    }
   ],
   "source": [
    "# final answer\n",
    "def format_qa_pairs(question, answer):\n",
    "    \"\"\"\n",
    "    格式化问题和答案对\n",
    "    \"\"\"\n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "context = format_qa_pairs(questions, answers)\n",
    "\n",
    "# prompt\n",
    "template = \"\"\"Here is a set of Q+A pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize an answer to the question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_answer = final_rag_chain.invoke({\"question\": question, \"context\": context})\n",
    "pprint(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf865b79",
   "metadata": {},
   "source": [
    "总结：\n",
    "感觉递归的回答效果要好一些"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaf1159",
   "metadata": {},
   "source": [
    "# Part 8: Step Back（回退）\n",
    "之前的策略是使用更“具体”的问题进行检索，这里反其道而行，使用更“抽象”的问题进行检索，能对问题有一个“更高层次”的认识。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f88eb3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60733428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777f360b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55694c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
