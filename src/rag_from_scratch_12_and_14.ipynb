{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b05d553a",
   "metadata": {},
   "source": [
    "# Part 12: 多重表征索引\n",
    "主要的思路：对文档进行摘要，通过摘要进行索引。可以通过相似的逻辑，扩展对原文档的多种索引方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c3be0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# 加载网页数据\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\")\n",
    "docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d31d0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过llm对文档进行摘要\n",
    "import uuid\n",
    "import os\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import init_chat_model, ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "api_url = os.getenv('API_URL')\n",
    "api_key = os.getenv('API_KEY')\n",
    "model_name = os.getenv('MODEL')\n",
    "llm = init_chat_model(\n",
    "    model_provider=\"openai\",  # 避免langchain根据模型名自动选择供应商\n",
    "    model=model_name,\n",
    "    # temperature=0.0,\n",
    "    api_key=api_key,\n",
    "    base_url=api_url,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0b41f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给文档生成摘要\n",
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "summaries = chain.batch(docs, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93edd44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liutingting\\AppData\\Local\\Temp\\ipykernel_11820\\1688003342.py:15: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(collection_name=\"summaries\",\n",
      "f:\\project\\rag-from-scratch\\.venv\\Lib\\site-packages\\chromadb\\execution\\expression\\operator.py:225: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  \"\"\"Field proxy for building Where conditions with operator overloading.\n"
     ]
    }
   ],
   "source": [
    "# 使用摘要进行索引\n",
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from ark_embedding import ArkEmbeddings\n",
    "\n",
    "\n",
    "embd = ArkEmbeddings(\n",
    "    model=os.getenv(\"ALIYUN_EMBEDDING_MODEL\"),\n",
    "    api_key=os.getenv(\"ALIYUN_API_KEY\"),\n",
    "    api_url=os.getenv(\"ALIYUN_API_URL\"),\n",
    "    batch_size=10\n",
    ")\n",
    "# 向量化并存储\n",
    "vectorstore = Chroma(collection_name=\"summaries\",\n",
    "                     embedding_function=embd)\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# 构建retriever, 通过id_key关联向量和doc\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "\n",
    "# 与doc关联的摘要\n",
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]\n",
    "\n",
    "# 分别添加摘要(并向量化)和文档\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce5216bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'doc_id': '625d221f-ab8e-48eb-866f-7dd58760a6d0'}, page_content='Of course. Here is a summary of the document \"LLM Powered Autonomous Agents\" by Lilian Weng.\\n\\n### Document Summary\\n\\nThis comprehensive blog post explores the architecture, components, and real-world applications of autonomous agents powered by Large Language Models (LLMs). It frames the LLM as the core \"brain\" of an agent system, which is augmented by three key components to overcome its inherent limitations.\\n\\n#### Core Components of an LLM Agent:\\n\\n1.  **Planning:** The agent breaks down complex tasks into smaller, manageable subgoals and can self-reflect to learn from mistakes.\\n    *   **Task Decomposition:** Techniques like Chain-of-Thought (CoT) and Tree of Thoughts are used to break problems into steps.\\n    *   **Self-Reflection:** Frameworks like **ReAct** (Reason + Act) and **Reflexion** allow the agent to critique its past actions, learn from failures, and refine its future strategy.\\n\\n2.  **Memory:** The agent uses different types of memory to retain information.\\n    *   **Short-Term Memory:** Analogous to the model\\'s limited context window, used for in-context learning.\\n    *   **Long-Term Memory:** An external vector database (e.g., using **FAISS, ScaNN, or HNSW** for fast retrieval) that allows the agent to store and recall vast amounts of information over time.\\n\\n3.  **Tool Use:** The agent learns to call external APIs and tools to access information not contained in its pre-trained weights (e.g., current data, calculators, code execution, search engines).\\n    *   Projects like **MRKL**, **Toolformer**, and **HuggingGPT** demonstrate this capability, where the LLM acts as a router to select the right expert tool for a given task.\\n\\n#### Case Studies & Examples:\\n\\n*   **Scientific Discovery (ChemCrow):** An agent equipped with 13 expert chemistry tools that outperformed raw GPT-4 in designing and planning complex experiments, as judged by human experts.\\n*   **Generative Agents:** A simulation where 25 LLM-powered agents live in a sandbox environment (like The Sims), exhibiting believable human-like behaviors, forming relationships, and coordinating social events based on memory and reflection.\\n*   **Proof-of-Concepts:** The post examines the inner workings of popular early agent projects like **AutoGPT** and **GPT-Engineer**, highlighting their use of prompting, task clarification, and code generation.\\n\\n#### Key Challenges:\\n\\nThe post concludes by outlining several significant challenges that remain:\\n*   **Finite Context Length:** The LLM\\'s limited context window restricts historical detail and complex planning.\\n*   **Difficulty with Long-Term Planning:** Agents struggle to adjust plans robustly in the face of unexpected errors.\\n*   **Unreliability of Natural Language Interface:** Outputs can be inconsistently formatted or unpredictable, requiring extensive parsing and error-handling code.\\n\\nIn essence, the document provides a detailed technical blueprint for building LLM-based autonomous agents, illustrating their potential with real-world examples while frankly discussing the current limitations that need to be overcome.')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 匹配相似摘要\n",
    "query = \"Memory in agents\"\n",
    "sub_docs = vectorstore.similarity_search(query, k=1)\n",
    "sub_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51c4bb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liutingting\\AppData\\Local\\Temp\\ipykernel_11820\\1266415567.py:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(query, n_results=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\n\\nLLM Powered Autonomous Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\\nComponent Three:\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通过匹配摘要，检索相似文档\n",
    "retrieved_docs = retriever.get_relevant_documents(query, n_results=1)\n",
    "retrieved_docs[0].page_content[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c14b48",
   "metadata": {},
   "source": [
    "# Part 13: RAPTOR\n",
    "Recursive Abstractive Processing for Tree-Organized Retrieval  \n",
    "参考代码：https://github.com/parthsarthi03/raptor#  \n",
    "主要的思路：对聚类后的文本块进行摘要并嵌入，递归这个过程，自底向上构建具有树状结构的不同层级摘要和嵌入。在推理时，从该树中进行检索，整合长篇文档中不同抽象层级的信息。 \n",
    "整体的思想，有点类似GraphRAG的分层聚类，获取不同层级的信息，只是GraphRAG是对知识图谱进行操作，而RAPTOR是直接对分块chunk或文档进行操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c5b333a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/young/project/llmProject/rag-from-scratch/raptor\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = (Path(os.getcwd()).parent / \"raptor\").resolve().as_posix()\n",
    "sys.path.append(project_root)\n",
    "print(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57f59be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/young/project/llmProject/rag-from-scratch/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-11-22 16:01:52,889 - Loading faiss.\n",
      "2025-11-22 16:01:54,452 - Successfully loaded faiss.\n"
     ]
    }
   ],
   "source": [
    "from raptor import (\n",
    "    BaseSummarizationModel, \n",
    "    BaseQAModel, \n",
    "    BaseEmbeddingModel, \n",
    "    RetrievalAugmentation,\n",
    "    RetrievalAugmentationConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9877e59c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac57e136",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySummarizationModel(BaseSummarizationModel):\n",
    "    def __init__(self, model=os.environ[\"MODEL\"]):\n",
    "        self.model = os.environ[\"MODEL\"]\n",
    "\n",
    "    # @retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "    def summarize(self, context, max_tokens=500, stop_sequence=None):\n",
    "\n",
    "        try:\n",
    "            client = OpenAI(\n",
    "                base_url=os.environ[\"API_URL\"],\n",
    "                api_key=os.environ[\"API_KEY\"],\n",
    "            )\n",
    "\n",
    "            response = client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"Write a summary of the following, including as many key details as possible: {context}:\",\n",
    "                    },\n",
    "                ],\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "\n",
    "            return response.choices[0].message.content\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"error:\", e)\n",
    "            return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "079d9833",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyQAModel(BaseQAModel):\n",
    "    def __init__(self, model=os.environ[\"MODEL\"]):\n",
    "        \"\"\"\n",
    "        Initializes the GPT-3 model with the specified model version.\n",
    "\n",
    "        Args:\n",
    "            model (str, optional): The GPT-3 model version to use for generating summaries. Defaults to \"text-davinci-003\".\n",
    "        \"\"\"\n",
    "        self.model = os.environ[\"MODEL\"]\n",
    "        self.client = OpenAI(\n",
    "            api_key=os.environ[\"API_KEY\"],\n",
    "            base_url=os.environ[\"API_URL\"],\n",
    "        )\n",
    "\n",
    "    # @retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "    def answer_question(self, context, question, max_tokens=150, stop_sequence=None):\n",
    "        \"\"\"\n",
    "        Generates a summary of the given context using the GPT-3 model.\n",
    "\n",
    "        Args:\n",
    "            context (str): The text to summarize.\n",
    "            max_tokens (int, optional): The maximum number of tokens in the generated summary. Defaults to 150.\n",
    "            stop_sequence (str, optional): The sequence at which to stop summarization. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated summary.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # response = self.client.completions.create(\n",
    "            #     prompt=f\"using the folloing information {context}. Answer the following question in less than 5-7 words, if possible: {question}\",\n",
    "            #     temperature=0,\n",
    "            #     max_tokens=max_tokens,\n",
    "            #     top_p=1,\n",
    "            #     frequency_penalty=0,\n",
    "            #     presence_penalty=0,\n",
    "            #     stop=stop_sequence,\n",
    "            #     model=self.model,\n",
    "            # )\n",
    "            # return response.choices[0].text.strip()\n",
    "\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are Question Answering Portal.\"},\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"using the folloing information {context}. Answer the following question in less than 5-7 words, if possible: {question}\",\n",
    "                    },\n",
    "                ],\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "\n",
    "            return response.choices[0].message.content\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"error:\", e)\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8058118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ark_embedding import ArkEmbeddings\n",
    "\n",
    "\n",
    "class MyEmbeddingModel(BaseEmbeddingModel):\n",
    "    def __init__(self, model=\"text-embedding-ada-002\"):\n",
    "        self.client = ArkEmbeddings(\n",
    "            model=os.getenv(\"ALIYUN_EMBEDDING_MODEL\"),\n",
    "            api_key=os.getenv(\"ALIYUN_API_KEY\"),\n",
    "            api_url=os.getenv(\"ALIYUN_API_URL\"),\n",
    "            batch_size=10\n",
    "        )\n",
    "\n",
    "    # @retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "    def create_embedding(self, text):\n",
    "        try:\n",
    "            text = text.replace(\"\\n\", \" \")\n",
    "            embd = self.client.embed_documents([text])\n",
    "            return embd[0]\n",
    "        except Exception as e:\n",
    "            print(\"error:\", e)\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "485130d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAC = RetrievalAugmentationConfig(\n",
    "    summarization_model=MySummarizationModel(), \n",
    "    qa_model=MyQAModel(), \n",
    "    embedding_model=MyEmbeddingModel()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b31fc3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The wife of a rich man fell sick, and as she felt that her end\n",
      "was drawing near, she called her only\n"
     ]
    }
   ],
   "source": [
    "# 加载测试用文本\n",
    "with open('data/sample.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0fa7617",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-22 16:49:47,763 - Successfully initialized TreeBuilder with Config \n",
      "        TreeBuilderConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Max Tokens: 100\n",
      "            Num Layers: 5\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Summarization Length: 100\n",
      "            Summarization Model: <__main__.MySummarizationModel object at 0x30e9f5880>\n",
      "            Embedding Models: {'EMB': <__main__.MyEmbeddingModel object at 0x10ba10da0>}\n",
      "            Cluster Embedding Model: EMB\n",
      "        \n",
      "        Reduction Dimension: 10\n",
      "        Clustering Algorithm: RAPTOR_Clustering\n",
      "        Clustering Parameters: {}\n",
      "        \n",
      "2025-11-22 16:49:47,764 - Successfully initialized ClusterTreeBuilder with Config \n",
      "        TreeBuilderConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Max Tokens: 100\n",
      "            Num Layers: 5\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Summarization Length: 100\n",
      "            Summarization Model: <__main__.MySummarizationModel object at 0x30e9f5880>\n",
      "            Embedding Models: {'EMB': <__main__.MyEmbeddingModel object at 0x10ba10da0>}\n",
      "            Cluster Embedding Model: EMB\n",
      "        \n",
      "        Reduction Dimension: 10\n",
      "        Clustering Algorithm: RAPTOR_Clustering\n",
      "        Clustering Parameters: {}\n",
      "        \n",
      "2025-11-22 16:49:47,764 - Successfully initialized RetrievalAugmentation with Config \n",
      "        RetrievalAugmentationConfig:\n",
      "            \n",
      "        TreeBuilderConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Max Tokens: 100\n",
      "            Num Layers: 5\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Summarization Length: 100\n",
      "            Summarization Model: <__main__.MySummarizationModel object at 0x30e9f5880>\n",
      "            Embedding Models: {'EMB': <__main__.MyEmbeddingModel object at 0x10ba10da0>}\n",
      "            Cluster Embedding Model: EMB\n",
      "        \n",
      "        Reduction Dimension: 10\n",
      "        Clustering Algorithm: RAPTOR_Clustering\n",
      "        Clustering Parameters: {}\n",
      "        \n",
      "            \n",
      "            \n",
      "        TreeRetrieverConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Context Embedding Model: EMB\n",
      "            Embedding Model: <__main__.MyEmbeddingModel object at 0x10ba10da0>\n",
      "            Num Layers: None\n",
      "            Start Layer: None\n",
      "        \n",
      "            \n",
      "            QA Model: <__main__.MyQAModel object at 0x355345c70>\n",
      "            Tree Builder Type: cluster\n",
      "        \n",
      "2025-11-22 16:49:47,769 - Creating Leaf Nodes\n",
      "2025-11-22 16:49:47,919 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:47,922 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:47,937 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:47,937 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:47,938 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:47,938 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:47,949 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:47,951 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:47,953 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:47,971 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:47,974 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:47,979 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:48,000 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:48,003 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:48,004 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:48,028 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:48,028 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:48,035 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:48,039 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:48,045 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:48,047 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:48,060 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:48,063 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:48,066 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:48,074 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:48,074 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:48,094 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:48,105 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:48,109 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:48,112 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:48,125 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:48,127 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:48,136 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:48,146 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:48,159 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:48,161 - Created 35 Leaf Embeddings\n",
      "2025-11-22 16:49:48,161 - Building All Nodes\n",
      "2025-11-22 16:49:48,167 - Using Cluster TreeBuilder\n",
      "2025-11-22 16:49:48,167 - Constructing Layer 0\n",
      "/Users/young/project/llmProject/rag-from-scratch/.venv/lib/python3.12/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "2025-11-22 16:49:49,150 - Summarization Length: 100\n",
      "2025-11-22 16:49:52,107 - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:52,109 - Node Texts Length: 876, Summarized Text Length: 99\n",
      "2025-11-22 16:49:52,201 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:55,170 - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:55,171 - Node Texts Length: 407, Summarized Text Length: 102\n",
      "2025-11-22 16:49:55,379 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:58,672 - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:49:58,676 - Node Texts Length: 480, Summarized Text Length: 101\n",
      "2025-11-22 16:49:58,806 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:50:03,018 - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:50:03,020 - Node Texts Length: 766, Summarized Text Length: 102\n",
      "2025-11-22 16:50:03,129 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:50:06,130 - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:50:06,134 - Node Texts Length: 463, Summarized Text Length: 101\n",
      "2025-11-22 16:50:06,274 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:50:10,293 - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:50:10,306 - Node Texts Length: 289, Summarized Text Length: 100\n",
      "2025-11-22 16:50:10,491 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:50:10,495 - Constructing Layer 1\n",
      "2025-11-22 16:50:10,496 - Stopping Layer construction: Cannot Create More Layers. Total Layers in tree: 1\n",
      "2025-11-22 16:50:10,497 - Successfully initialized TreeRetriever with Config \n",
      "        TreeRetrieverConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Context Embedding Model: EMB\n",
      "            Embedding Model: <__main__.MyEmbeddingModel object at 0x10ba10da0>\n",
      "            Num Layers: None\n",
      "            Start Layer: None\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# 构建树状检索\n",
    "RA = RetrievalAugmentation(config=RAC)\n",
    "\n",
    "RA.add_documents(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff7117dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-22 16:50:10,506 - Using collapsed_tree\n",
      "2025-11-22 16:50:10,640 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:50:11,908 - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  By marrying the prince.\n"
     ]
    }
   ],
   "source": [
    "# 从树状检索中查询\n",
    "question = \"How did Cinderella reach her happy ending?\"\n",
    "\n",
    "answer = RA.answer_question(question=question)\n",
    "\n",
    "print(\"Answer: \", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "86f9bce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-22 16:50:27,262 - Tree successfully saved to data/cinderella\n"
     ]
    }
   ],
   "source": [
    "# 保存结果\n",
    "SAVE_PATH = \"data/cinderella\"\n",
    "RA.save(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08e18b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-22 16:50:27,564 - Successfully initialized TreeBuilder with Config \n",
      "        TreeBuilderConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Max Tokens: 100\n",
      "            Num Layers: 5\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Summarization Length: 100\n",
      "            Summarization Model: <__main__.MySummarizationModel object at 0x30e9f5880>\n",
      "            Embedding Models: {'EMB': <__main__.MyEmbeddingModel object at 0x10ba10da0>}\n",
      "            Cluster Embedding Model: EMB\n",
      "        \n",
      "        Reduction Dimension: 10\n",
      "        Clustering Algorithm: RAPTOR_Clustering\n",
      "        Clustering Parameters: {}\n",
      "        \n",
      "2025-11-22 16:50:27,564 - Successfully initialized ClusterTreeBuilder with Config \n",
      "        TreeBuilderConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Max Tokens: 100\n",
      "            Num Layers: 5\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Summarization Length: 100\n",
      "            Summarization Model: <__main__.MySummarizationModel object at 0x30e9f5880>\n",
      "            Embedding Models: {'EMB': <__main__.MyEmbeddingModel object at 0x10ba10da0>}\n",
      "            Cluster Embedding Model: EMB\n",
      "        \n",
      "        Reduction Dimension: 10\n",
      "        Clustering Algorithm: RAPTOR_Clustering\n",
      "        Clustering Parameters: {}\n",
      "        \n",
      "2025-11-22 16:50:27,564 - Successfully initialized TreeRetriever with Config \n",
      "        TreeRetrieverConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Context Embedding Model: EMB\n",
      "            Embedding Model: <__main__.MyEmbeddingModel object at 0x10ba10da0>\n",
      "            Num Layers: None\n",
      "            Start Layer: None\n",
      "        \n",
      "2025-11-22 16:50:27,565 - Successfully initialized RetrievalAugmentation with Config \n",
      "        RetrievalAugmentationConfig:\n",
      "            \n",
      "        TreeBuilderConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Max Tokens: 100\n",
      "            Num Layers: 5\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Summarization Length: 100\n",
      "            Summarization Model: <__main__.MySummarizationModel object at 0x30e9f5880>\n",
      "            Embedding Models: {'EMB': <__main__.MyEmbeddingModel object at 0x10ba10da0>}\n",
      "            Cluster Embedding Model: EMB\n",
      "        \n",
      "        Reduction Dimension: 10\n",
      "        Clustering Algorithm: RAPTOR_Clustering\n",
      "        Clustering Parameters: {}\n",
      "        \n",
      "            \n",
      "            \n",
      "        TreeRetrieverConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Context Embedding Model: EMB\n",
      "            Embedding Model: <__main__.MyEmbeddingModel object at 0x10ba10da0>\n",
      "            Num Layers: None\n",
      "            Start Layer: None\n",
      "        \n",
      "            \n",
      "            QA Model: <__main__.MyQAModel object at 0x355345c70>\n",
      "            Tree Builder Type: cluster\n",
      "        \n",
      "2025-11-22 16:50:27,565 - Using collapsed_tree\n",
      "2025-11-22 16:50:27,716 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-22 16:50:28,591 - HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  With the prince's help.\n"
     ]
    }
   ],
   "source": [
    "# 从保存的结果中恢复检索结果\n",
    "RA = RetrievalAugmentation(config=RAC, tree=SAVE_PATH)\n",
    "\n",
    "answer = RA.answer_question(question=question)\n",
    "print(\"Answer: \", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d61552c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17a8d9a7",
   "metadata": {},
   "source": [
    "# Part 14: ColBERT\n",
    "介绍资料：https://hackernoon.com/how-colbert-helps-developers-overcome-the-limits-of-rag  \n",
    "论文：https://arxiv.org/abs/2004.12832?ref=hackernoon.com  \n",
    "核心原理：\n",
    "- 通过分词+bert进行向量化（通过双向的transformer编码，得到考虑了上下文的向量）。\n",
    "- 文档和查询都会做相同的处理。\n",
    "- 每个文档的总得分 = 逐个“分词后的查询向量”分别计算与“分词后的文档向量”的最大相似度，并在“分词后的查询向量”粒度求和。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56cea599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7r/h44tfn_x35jg4s7msmb1gdzw0000gp/T/ipykernel_29525/1502630049.py:2: UserWarning: \n",
      "********************************************************************************\n",
      "RAGatouille WARNING: Future Release Notice\n",
      "--------------------------------------------\n",
      "RAGatouille version 0.0.10 will be migrating to a PyLate backend \n",
      "instead of the current Stanford ColBERT backend.\n",
      "PyLate is a fully mature, feature-equivalent backend, that greatly facilitates compatibility.\n",
      "However, please pin version <0.0.10 if you require the Stanford ColBERT backend.\n",
      "********************************************************************************\n",
      "  from ragatouille import RAGPretrainedModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Nov 22, 16:32:53] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/young/project/llmProject/rag-from-scratch/.venv/lib/python3.12/site-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n",
      "/Users/young/project/llmProject/rag-from-scratch/.venv/lib/python3.12/site-packages/torch/cuda/amp/grad_scaler.py:31: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  super().__init__(\n"
     ]
    }
   ],
   "source": [
    "# 使用专用于colbert的模型\n",
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3cc4157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载wiki的数据\n",
    "import requests\n",
    "\n",
    "def get_wikipedia_page(title: str):\n",
    "    \"\"\"\n",
    "    Retrieve the full text content of a Wikipedia page.\n",
    "\n",
    "    :param title: str - Title of the Wikipedia page.\n",
    "    :return: str - Full text content of the page as raw string.\n",
    "    \"\"\"\n",
    "    # Wikipedia API endpoint\n",
    "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    # Parameters for the API request\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": title,\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True,\n",
    "    }\n",
    "\n",
    "    # Custom User-Agent header to comply with Wikipedia's best practices\n",
    "    headers = {\"User-Agent\": \"RAGatouille_tutorial/0.0.1 (ben@clavie.eu)\"}\n",
    "\n",
    "    response = requests.get(URL, params=params, headers=headers)\n",
    "    data = response.json()\n",
    "\n",
    "    # Extracting page content\n",
    "    page = next(iter(data[\"query\"][\"pages\"].values()))\n",
    "    return page[\"extract\"] if \"extract\" in page else None\n",
    "\n",
    "full_document = get_wikipedia_page(\"Hayao_Miyazaki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc1ffcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- WARNING! You are using PLAID with an experimental replacement for FAISS for greater compatibility ----\n",
      "This is a behaviour change from RAGatouille 0.8.0 onwards.\n",
      "This works fine for most users and smallish datasets, but can be considerably slower than FAISS and could cause worse results in some situations.\n",
      "If you're confident with FAISS working on your machine, pass use_faiss=True to revert to the FAISS-using behaviour.\n",
      "--------------------\n",
      "\n",
      "\n",
      "[Nov 22, 16:51:05] #> Creating directory .ragatouille/colbert/indexes/Miyazaki-123 \n",
      "\n",
      "\n",
      "[Nov 22, 16:51:09] [0] \t\t #> Encoding 124 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/young/project/llmProject/rag-from-scratch/.venv/lib/python3.12/site-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n",
      "/Users/young/project/llmProject/rag-from-scratch/.venv/lib/python3.12/site-packages/torch/cuda/amp/grad_scaler.py:31: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  super().__init__(\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]/Users/young/project/llmProject/rag-from-scratch/.venv/lib/python3.12/site-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
      "/Users/young/project/llmProject/rag-from-scratch/.venv/lib/python3.12/site-packages/torch/amp/autocast_mode.py:270: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Nov 22, 16:51:12] [0] \t\t avg_doclen_est = 132.18548583984375 \t len(local_sample) = 124\n",
      "[Nov 22, 16:51:12] [0] \t\t Creating 2,048 partitions.\n",
      "[Nov 22, 16:51:12] [0] \t\t *Estimated* 16,391 embeddings.\n",
      "[Nov 22, 16:51:12] [0] \t\t #> Saving the indexing plan to .ragatouille/colbert/indexes/Miyazaki-123/plan.json ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: number of training points (15572) is less than the minimum recommended (20480)\n",
      "used 13 iterations (0.5391s) to cluster 15572 items into 2048 clusters\n",
      "[0.036, 0.038, 0.039, 0.033, 0.031, 0.035, 0.034, 0.034, 0.033, 0.033, 0.034, 0.035, 0.031, 0.035, 0.033, 0.035, 0.03, 0.032, 0.033, 0.036, 0.035, 0.034, 0.033, 0.035, 0.034, 0.031, 0.037, 0.032, 0.033, 0.035, 0.034, 0.036, 0.036, 0.033, 0.034, 0.03, 0.033, 0.032, 0.034, 0.039, 0.035, 0.037, 0.034, 0.032, 0.033, 0.033, 0.035, 0.036, 0.036, 0.031, 0.033, 0.033, 0.032, 0.033, 0.034, 0.033, 0.036, 0.035, 0.04, 0.031, 0.032, 0.032, 0.035, 0.032, 0.037, 0.034, 0.033, 0.036, 0.033, 0.031, 0.035, 0.033, 0.033, 0.035, 0.034, 0.032, 0.033, 0.037, 0.032, 0.033, 0.035, 0.036, 0.031, 0.036, 0.031, 0.034, 0.035, 0.035, 0.032, 0.037, 0.034, 0.036, 0.033, 0.035, 0.035, 0.035, 0.036, 0.033, 0.036, 0.034, 0.037, 0.039, 0.036, 0.035, 0.037, 0.033, 0.034, 0.032, 0.035, 0.031, 0.035, 0.035, 0.034, 0.031, 0.035, 0.034, 0.034, 0.034, 0.036, 0.035, 0.03, 0.032, 0.033, 0.035, 0.032, 0.033, 0.034, 0.035]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Nov 22, 16:51:13] [0] \t\t #> Encoding 124 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:02<00:00,  1.53it/s]\n",
      "1it [00:02,  2.66s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1651.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Nov 22, 16:51:16] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Nov 22, 16:51:16] #> Building the emb2pid mapping..\n",
      "[Nov 22, 16:51:16] len(emb2pid) = 16391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2048/2048 [00:00<00:00, 196148.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Nov 22, 16:51:16] #> Saved optimized IVF to .ragatouille/colbert/indexes/Miyazaki-123/ivf.pid.pt\n",
      "Done indexing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'.ragatouille/colbert/indexes/Miyazaki-123'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 建立索引\n",
    "RAG.index(\n",
    "    collection=[full_document],\n",
    "    index_name=\"Miyazaki-123\",\n",
    "    max_document_length=180,\n",
    "    split_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b844645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': '=== Studio Ghibli ===\\n\\n\\n==== Foundation and Laputa (1985–1987) ====\\n\\nFollowing the success of Nausicaä of the Valley of the Wind, Miyazaki and Takahata founded the animation production company Studio Ghibli on June 15, 1985, as a subsidiary of Tokuma Shoten, with offices in Kichijōji designed by Miyazaki. The studio\\'s name had been registered a year earlier; Miyazaki named it after the nickname of the Caproni Ca.309 aircraft, meaning \"a hot wind that blows in the desert\" in Italian.',\n",
       "  'score': 25.73267364501953,\n",
       "  'rank': 1,\n",
       "  'document_id': 'c39b400c-b67d-4fa7-b303-b9a7ea91925b',\n",
       "  'passage_id': 42},\n",
       " {'content': 'Hayao Miyazaki (宮崎 駿 or 宮﨑 駿, Miyazaki Hayao; [mijaꜜzaki hajao]; born January 5, 1941) is a Japanese animator, filmmaker, and manga artist. He co-founded Studio Ghibli and serves as its honorary chairman. Throughout his career, Miyazaki has attained international acclaim as a masterful storyteller and creator of Japanese animated feature films, and is widely regarded as one of the most accomplished filmmakers in the history of animation.\\nBorn in Tokyo City, Miyazaki expressed interest in manga and animation from an early age.',\n",
       "  'score': 25.507200241088867,\n",
       "  'rank': 2,\n",
       "  'document_id': 'c39b400c-b67d-4fa7-b303-b9a7ea91925b',\n",
       "  'passage_id': 0},\n",
       " {'content': \"Miyazaki, initially reluctant, countered that an hour-long animation would be more suitable, and Tokuma Shoten agreed on a feature-length film.\\nProduction began on May 31, 1983, with animation beginning in August; funding was provided through a joint venture between Tokuma Shoten and the advertising agency Hakuhodo, for whom Miyazaki's youngest brother worked. Animation studio Topcraft was chosen as the production house. Miyazaki found some of Topcraft's staff unreliable, and brought on several of his previous collaborators, including Takahata, who served as producer, though he was reluctant to do so. Pre-production began on May 31, 1983; Miyazaki encountered difficulties in creating the screenplay, with only sixteen chapters of the manga to work with.\",\n",
       "  'score': 25.265661239624023,\n",
       "  'rank': 3,\n",
       "  'document_id': 'c39b400c-b67d-4fa7-b303-b9a7ea91925b',\n",
       "  'passage_id': 38}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检索\n",
    "results = RAG.search(query=\"What animation studio did Miyazaki found?\", k=3)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42fe8790",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/young/project/llmProject/rag-from-scratch/.venv/lib/python3.12/site-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
      "/Users/young/project/llmProject/rag-from-scratch/.venv/lib/python3.12/site-packages/torch/amp/autocast_mode.py:270: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='=== Studio Ghibli ===\\n\\n\\n==== Foundation and Laputa (1985–1987) ====\\n\\nFollowing the success of Nausicaä of the Valley of the Wind, Miyazaki and Takahata founded the animation production company Studio Ghibli on June 15, 1985, as a subsidiary of Tokuma Shoten, with offices in Kichijōji designed by Miyazaki. The studio\\'s name had been registered a year earlier; Miyazaki named it after the nickname of the Caproni Ca.309 aircraft, meaning \"a hot wind that blows in the desert\" in Italian.'),\n",
       " Document(metadata={}, page_content='Hayao Miyazaki (宮崎 駿 or 宮﨑 駿, Miyazaki Hayao; [mijaꜜzaki hajao]; born January 5, 1941) is a Japanese animator, filmmaker, and manga artist. He co-founded Studio Ghibli and serves as its honorary chairman. Throughout his career, Miyazaki has attained international acclaim as a masterful storyteller and creator of Japanese animated feature films, and is widely regarded as one of the most accomplished filmmakers in the history of animation.\\nBorn in Tokyo City, Miyazaki expressed interest in manga and animation from an early age.'),\n",
       " Document(metadata={}, page_content=\"Miyazaki, initially reluctant, countered that an hour-long animation would be more suitable, and Tokuma Shoten agreed on a feature-length film.\\nProduction began on May 31, 1983, with animation beginning in August; funding was provided through a joint venture between Tokuma Shoten and the advertising agency Hakuhodo, for whom Miyazaki's youngest brother worked. Animation studio Topcraft was chosen as the production house. Miyazaki found some of Topcraft's staff unreliable, and brought on several of his previous collaborators, including Takahata, who served as producer, though he was reluctant to do so. Pre-production began on May 31, 1983; Miyazaki encountered difficulties in creating the screenplay, with only sixteen chapters of the manga to work with.\")]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转换为langchain retriever\n",
    "retriever = RAG.as_langchain_retriever(k=3)\n",
    "retriever.invoke(\"What animation studio did Miyazaki found?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea58badb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
