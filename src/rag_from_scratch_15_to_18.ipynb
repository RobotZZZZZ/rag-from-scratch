{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf026d5e",
   "metadata": {},
   "source": [
    "# Part 15: Re-ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8f76e4",
   "metadata": {},
   "source": [
    "### 根据rag-fusion进行索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8251de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载并索引数据\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()\n",
    "\n",
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "# Index\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "# from langchain_cohere import CohereEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    # embedding=CohereEmbeddings()\n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad72c094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由llm生成多个query\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# RAG-Fusion\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b821a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 请求llm生成多个query，用于rag-fusion\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "api_url = os.getenv('API_URL')\n",
    "api_key = os.getenv('API_KEY')\n",
    "model_name = os.getenv('MODEL')\n",
    "llm = init_chat_model(\n",
    "    model_provider=\"openai\",  # 避免langchain根据模型名自动选择供应商\n",
    "    model=model_name,\n",
    "    # temperature=0.0,\n",
    "    api_key=api_key,\n",
    "    base_url=api_url,\n",
    ")\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdb0e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将多个query的检索结果，通过rrf打分，进行排序（与rag-fusion部分逻辑完全一样）\n",
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23520e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于fusion的结果进行回答\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bc3e81",
   "metadata": {},
   "source": [
    "### 使用CohereRerank进行重排序\n",
    "介绍链接：https://cohere.com/blog/rerank  \n",
    "langchain介绍文档：https://docs.langchain.com/oss/python/integrations/retrievers/cohere-reranker#doing-reranking-with-coherererank  \n",
    "**核心思想**：将检索到的doc，通过“排序模型”计算查询和doc的相似度进行排序（类似召回和排序的逻辑），保留topk的结果，提升问答效果。  \n",
    "**优势**：排序模型可以做得更加复杂，相关性计算更加准确。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8807533",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Cohere\n",
    "from langchain.retrievers import  ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b19ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "# Re-rank\n",
    "compressor = CohereRerank()\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953aa14b",
   "metadata": {},
   "source": [
    "# Part 16 - Retrieval (CRAG)\n",
    "**核心思路**：通过构建工作流，让llm对检索结果进行反思，判断当前的检索结果是否满足需求，当质量或信息不满足回答问题的需要时，会使用web search来获取补充信息。  \n",
    "参考代码：https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_crag.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14ddc07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3aa35b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86573a04",
   "metadata": {},
   "source": [
    "# Part 17 - Retrieval (Self-RAG)\n",
    "**核心思路**：通过构建工作流，让llm对检索结果进行反思，判断当前的检索结果是否满足需求，当质量或信息不满足回答问题的需要时，改写query再次进行检索。  \n",
    "参考代码：https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_self_rag.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47573d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ed5be2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b9e443e",
   "metadata": {},
   "source": [
    "# Part 18 - Impact of long context\n",
    "视频资料：https://www.youtube.com/watch?v=SsHUNfhF32s  \n",
    "PPT资料：https://docs.google.com/presentation/d/1mJUiPBdtf58NfuSEQ7pVSEQ2Oqmek7F1i4gBwR6JDss/edit?slide=id.g26c0cb8dc66_0_0#slide=id.g26c0cb8dc66_0_0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e5f937",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacb53ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
